# -*- coding: utf-8 -*-
"""news_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19PMbUVMgot5BtCG9Mbu0z3K0_4Ye7gI8

# Import libraries
"""

import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import os

nltk.download('stopwords')

"""# Step 1: Combine Fake and True datasets

"""

fake_path = "Fake.csv"
true_path = "True.csv"

if not (os.path.exists(fake_path) and os.path.exists(true_path)):
    raise FileNotFoundError("Fake.csv and/or True.csv files not found in the directory.")

print("Loading datasets...")
fake = pd.read_csv(fake_path)
true = pd.read_csv(true_path)

"""Add labels: 0 = Fake, 1 = Real

"""

fake['label'] = 0
true['label'] = 1

fake

true

"""Combine and shuffle

"""

df = pd.concat([fake, true], axis=0).sample(frac=1, random_state=42).reset_index(drop=True)

df

"""Keep only relevant columns

"""

if 'text' not in df.columns or 'title' not in df.columns:
    raise KeyError("The dataset must have 'title' and 'text' columns.")

df = df[['title', 'text', 'label']]

df

"""Save combined dataset

"""

df.to_csv("news.csv", index=False)
print("Combined dataset saved as news.csv")
print("Dataset Shape:", df.shape)
print(df.head())

"""# Step 2: Data Cleaning

"""

stop_words = set(stopwords.words('english'))
ps = PorterStemmer()

def clean_text(text):
    text = re.sub('[^a-zA-Z]', ' ', str(text))  # Remove non-alphabetic characters
    text = text.lower()
    words = text.split()
    words = [ps.stem(word) for word in words if word not in stop_words]
    return " ".join(words)

print("\nCleaning text data...")
df['clean_text'] = df['text'].apply(clean_text)
print("Sample cleaned text:")
print(df[['text', 'clean_text']].head())

"""# Step 3: TF-IDF Vectorization

"""

print("\nVectorizing text using TF-IDF...")
tfidf = TfidfVectorizer(max_features=5000)
X = tfidf.fit_transform(df['clean_text']).toarray()
y = df['label']

print("Shape of TF-IDF matrix:", X.shape)

df

"""# Step 4: Train-Test Split

"""

print("\nSplitting dataset...")
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
print("Training set size:", X_train.shape)
print("Test set size:", X_test.shape)

"""# Step 5: Train Models
 Logistic Regression

 Naive Bayes

"""

print("\nTraining models...")

# Logistic Regression
lr_model = LogisticRegression(max_iter=1000)
lr_model.fit(X_train, y_train)
y_pred_lr = lr_model.predict(X_test)

nb_model = MultinomialNB()
nb_model.fit(X_train, y_train)
y_pred_nb = nb_model.predict(X_test)

"""# Step 6: Evaluation

"""

print("\n--- Logistic Regression ---")
print("Accuracy:", accuracy_score(y_test, y_pred_lr))
print("F1 Score:", f1_score(y_test, y_pred_lr))
print("\nClassification Report:\n", classification_report(y_test, y_pred_lr))

print("\n--- Naive Bayes ---")
print("Accuracy:", accuracy_score(y_test, y_pred_nb))
print("F1 Score:", f1_score(y_test, y_pred_nb))
print("\nClassification Report:\n", classification_report(y_test, y_pred_nb))

"""# Confusion Matrix for Logistic Regression

"""

cm = confusion_matrix(y_test, y_pred_lr)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Logistic Regression")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""# Step 7: Save Model and Vectorizer

"""

joblib.dump(lr_model, "fake_news_lr_model.pkl")
joblib.dump(tfidf, "tfidf_vectorizer.pkl")
print("\nModel and TF-IDF vectorizer saved successfully!")